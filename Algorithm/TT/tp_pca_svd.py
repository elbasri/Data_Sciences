# -*- coding: utf-8 -*-
"""TP_PCA_SVD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uXlY6gKZbAXJHLw2xcYhwI-2pkjed3L1

# Principal Component Analysis: PCA svd

L'objectif est de réduire la dimension des données
Supposons qu'on a 10 vecteurs (Features) et chaque vecteur est composé de 6 élements

$$ X = \{ x^{(1)}, x^{(2)},...,x^{(10)} \}$$
avec
$$ x^{(i)} \in  \mathbb{R}^{6}  $$
Alors on doit trouver une représentation $ c^{(i)}$ de $ x^{(i)}$ tel que $$ c^{(i)} \in  \mathbb{R}^{4}  $$
afin de réduire la taille de $ x^{(i)}$ de 6 composants à 4 composants
c'est à dire trouver une matrice $ D $ telle que:
$$ c^{(i)} = D^T x^{(i)} $$
et $$ x^{(i)} = D c^{(i)} $$
$D$ est composée des vecteurs propres correspondants aux valeurs propres les plus importantes de $ X X^T$ $$ $$
Dans notre exemple $ X (6\times 10)$ alors $ X^T (10\times 6)$ $$ $$
Donc $ X X^T$  est une matrice carée de taille $6\times 6$ $$ $$
Alors pour réduire la taille de $ x^{(i)}$ de 6 composants à 4 composants on crée la matrice $D$ à partir les 4 importants vecteurs propres de $ X X^T$
"""

import numpy as np

"""# Matrice des données"""

X = np.random.rand(6,10)

x1 = X[:,0]
x1

r = np.linalg.matrix_rank(X)
r

np.mean(x1)

"""# Moyenne de chaque vecteur (colonne axe=0)"""

M = np.mean(X,axis=0)
M

"""Soustraire la moyenne de chaque vecteur $x^{(i)}$"""

X_mean0 = X-M

Mz = np.mean(X_mean0,axis=0)
Mz

"""# SVD  (Remarque:svd de numpy return $V^T$ et non pas $V$)"""

U,S,V = np.linalg.svd(X_mean0,full_matrices='true')
print(X_mean0.shape)
print(U.shape)
print(V.shape)
print(S)
S.shape
Ss = np.zeros((10,),'float')
for i in range(6):
    Ss[i] = S[i]
Ss

"""# Variance de $x$, $var[x] =\frac{1}{m-1}X^T X = \frac{1}{m-1} V S^2 V^T$"""

m=10
S2 = np.dot(np.diag(Ss),np.diag(Ss))
#print(S2)
var_x = (1/(1-m))*np.dot(np.dot(V.transpose(),S2),V)
print(var_x.shape)
#print(var_x)
print('==================================================================')
var2_x = (1/(1-m))*np.dot(X_mean0.transpose(),X_mean0)
print(var_x[5,2])
print(var2_x[5,2])
np.allclose(var_x,var2_x)









"""# Construire la matrice $D$"""

print(V.shape)
D = V.transpose()[:4,:]
print(D.shape)
print(np.dot(D,D.transpose()))

D

"""# On construit le vecteur c1 représentant de x1 $c^{(i)} = D^T x^{(i)}$"""





"""
# On reconstruit x1 à partir de c1 $x^{(i)} = D c^{(i)}$"""

x1_estim = np.dot(D.transpose(),c1)
print(x1_estim + M[0:6])
print(x1)

"""# Matrice des codes"""

C = np.dot(D.transpose(),X)
C

"""# Reconstruction de X"""

X_estim = np.dot(D,C)
print(X_estim)
print("---------------------------------------------------")
print(X)

